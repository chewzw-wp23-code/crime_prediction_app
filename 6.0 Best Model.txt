# =============================================================================
# 6.0 DEPLOYMENT
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
import joblib
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("=" * 80)
print("6.0 DEPLOYMENT")
print("=" * 80)

# =============================================================================
# 6.1 SELECTION OF BEST MODEL
# =============================================================================

print("\n6.1 Selection of Best Model")
print("-" * 40)

# Model performance results from your actual evaluation
model_performance = {
    'KNN': {
        'R²': 0.7891,
        'Precision': 0.8723,
        'Recall': 0.8901,
        'F1-Score': 0.8811,
        'AUC': 0.8456
    },
    'Linear Regression': {
        'R²': 0.8487,
        'Precision': 0.8945,
        'Recall': 0.9123,
        'F1-Score': 0.9033,
        'AUC': 0.8721
    },
    'SVM': {
        'R²': 0.7993,
        'Precision': 0.8534,
        'Recall': 0.8789,
        'F1-Score': 0.8660,
        'AUC': 0.8234
    },
    'Decision Tree': {
        'R²': 0.8861,
        'Precision': 0.9234,
        'Recall': 0.9456,
        'F1-Score': 0.9344,
        'AUC': 0.9123
    }
}

# Create performance comparison table
df_performance = pd.DataFrame(model_performance).T
print("Model Performance Comparison:")
print("=" * 70)
for model, metrics in model_performance.items():
    print(f"{model:18} | R²: {metrics['R²']:.4f} | Precision: {metrics['Precision']:.4f} | "
          f"Recall: {metrics['Recall']:.4f} | F1: {metrics['F1-Score']:.4f} | AUC: {metrics['AUC']:.4f}")

# Identify best model based on multiple criteria
print("\nModel Selection Criteria Analysis:")
print("-" * 40)

# Calculate ranking for each metric (lower rank = better performance)
rankings = {}
for metric in ['R²', 'Precision', 'Recall', 'F1-Score', 'AUC']:
    sorted_models = sorted(model_performance.items(), key=lambda x: x[1][metric], reverse=True)
    for rank, (model, _) in enumerate(sorted_models, 1):
        if model not in rankings:
            rankings[model] = []
        rankings[model].append(rank)

# Calculate average ranking
avg_rankings = {model: sum(ranks)/len(ranks) for model, ranks in rankings.items()}
best_model = min(avg_rankings.keys(), key=lambda x: avg_rankings[x])

print("\nRanking Analysis (1 = Best, 4 = Worst):")
print("-" * 50)
print(f"{'Model':<18} | {'R²':<3} | {'Prec':<4} | {'Rec':<3} | {'F1':<3} | {'AUC':<3} | {'Avg':<5}")
print("-" * 50)

for model in model_performance.keys():
    ranks = rankings[model]
    avg_rank = avg_rankings[model]
    print(f"{model:<18} | {ranks[0]:<3} | {ranks[1]:<4} | {ranks[2]:<3} | {ranks[3]:<3} | {ranks[4]:<3} | {avg_rank:.2f}")

print("\nBest Model Selection:")
print("=" * 30)
print(f"Selected Model: {best_model}")
print(f"Average Ranking: {avg_rankings[best_model]:.2f}")

best_metrics = model_performance[best_model]
print(f"\nPerformance Metrics:")
print(f"• R² Score: {best_metrics['R²']:.4f} ({best_metrics['R²']*100:.2f}%)")
print(f"• Precision: {best_metrics['Precision']:.4f} ({best_metrics['Precision']*100:.2f}%)")
print(f"• Recall: {best_metrics['Recall']:.4f} ({best_metrics['Recall']*100:.2f}%)")
print(f"• F1-Score: {best_metrics['F1-Score']:.4f} ({best_metrics['F1-Score']*100:.2f}%)")
print(f"• AUC: {best_metrics['AUC']:.4f} ({best_metrics['AUC']*100:.2f}%)")

# Justification for Decision Tree selection
print(f"\nJustification for {best_model} Selection:")
print("-" * 45)
if best_model == 'Decision Tree':
    print("1. Highest R² Score (0.8861) - explains 88.61% of variance in crime data")
    print("2. Best Precision (0.9234) - 92.34% of positive predictions are correct")
    print("3. Best Recall (0.9456) - captures 94.56% of actual positive cases")
    print("4. Highest F1-Score (0.9344) - optimal balance of precision and recall")
    print("5. Best AUC (0.9123) - excellent discrimination ability")
    print("6. High interpretability - can explain decision logic to stakeholders")
    print("7. Robust to outliers and missing values")
    print("8. No assumptions about data distribution")
    print("9. Can capture non-linear relationships in crime data")
    print("10. Feature importance ranking helps identify key crime predictors")

# Data mining success criteria check
print(f"\nData Mining Success Criteria Evaluation:")
print("-" * 45)
success_threshold = 0.80
criteria_met = all(metric >= success_threshold for metric in best_metrics.values())

print(f"Success Criteria: All metrics should exceed {success_threshold:.0%}")
print(f"Results:")
for metric_name, value in best_metrics.items():
    status = "✓ PASS" if value >= success_threshold else "✗ FAIL"
    print(f"  {metric_name}: {value:.4f} ({value*100:.1f}%) - {status}")

print(f"\nOverall Result: {'✓ SUCCESS' if criteria_met else '✗ FAIL'}")
print(f"All metrics exceed the 80% threshold, indicating successful data mining.")