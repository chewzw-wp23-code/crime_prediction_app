5.0 Evaluation
# =============================================================================
# SECTION 5.0 EVALUATION - COMPARISON CHARTS
# =============================================================================

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Model performance data (replace with your actual results)
model_results = {
    'Algorithm': ['KNN', 'Linear Regression', 'SVM', 'Decision Tree'],
    'R²': [0.7891, 0.8487, 0.7993, 0.8861],
    'MAE': [0.0847, 0.0756, 0.0823, 0.0634],
    'RMSE': [0.1113, 0.0943, 0.1086, 0.0819],
    'Precision': [0.8723, 0.8945, 0.8534, 0.9234],
    'Recall': [0.8901, 0.9123, 0.8789, 0.9456],
    'F1-Score': [0.8811, 0.9033, 0.8660, 0.9344],
    'AUC': [0.8456, 0.8721, 0.8234, 0.9123]
}

df_results = pd.DataFrame(model_results)

# Set consistent styling
plt.style.use('default')
colors = ['skyblue', 'lightgreen', 'salmon', 'gold']

# =============================================================================
# 5.1 ACCURACY COMPARISON (R² Score)
# =============================================================================

plt.figure(figsize=(10, 6))
bars = plt.bar(df_results['Algorithm'], df_results['R²'], color=colors)
plt.title('Comparing R² Score between Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('R² Score')
plt.ylim(0, 1)

# Add value labels on bars
for bar, value in zip(bars, df_results['R²']):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

# Add horizontal line at 0.8 (success criteria)
plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Success Criteria (80%)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# =============================================================================
# 5.2 PRECISION COMPARISON
# =============================================================================

plt.figure(figsize=(10, 6))
bars = plt.bar(df_results['Algorithm'], df_results['Precision'], color=colors)
plt.title('Comparing Precision between Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('Precision Score')
plt.ylim(0, 1)

# Add value labels on bars
for bar, value in zip(bars, df_results['Precision']):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Success Criteria (80%)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# =============================================================================
# 5.3 RECALL COMPARISON
# =============================================================================

plt.figure(figsize=(10, 6))
bars = plt.bar(df_results['Algorithm'], df_results['Recall'], color=colors)
plt.title('Comparing Recall between Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('Recall Score')
plt.ylim(0, 1)

# Add value labels on bars
for bar, value in zip(bars, df_results['Recall']):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Success Criteria (80%)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# =============================================================================
# 5.4 F1-SCORE COMPARISON
# =============================================================================

plt.figure(figsize=(10, 6))
bars = plt.bar(df_results['Algorithm'], df_results['F1-Score'], color=colors)
plt.title('Comparing F1-Score between Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('F1-Score')
plt.ylim(0, 1)

# Add value labels on bars
for bar, value in zip(bars, df_results['F1-Score']):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Success Criteria (80%)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# =============================================================================
# 5.5 AUC COMPARISON WITH PERFORMANCE GRADES
# =============================================================================

plt.figure(figsize=(10, 6))
bars = plt.bar(df_results['Algorithm'], df_results['AUC'], color=colors)
plt.title('Comparing Area Under ROC Curve (AUC) between Algorithms')
plt.xlabel('Algorithm')
plt.ylabel('AUC Score')
plt.ylim(0, 1)

# Add value labels on bars
for bar, value in zip(bars, df_results['AUC']):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.4f}', ha='center', va='bottom', fontweight='bold')

# Add performance grade lines
plt.axhline(y=0.9, color='green', linestyle='-', alpha=0.7, label='Excellent (≥0.90)')
plt.axhline(y=0.8, color='blue', linestyle='-', alpha=0.7, label='Good (0.80-0.89)')
plt.axhline(y=0.7, color='orange', linestyle='-', alpha=0.7, label='Fair (0.70-0.79)')
plt.axhline(y=0.6, color='red', linestyle='-', alpha=0.7, label='Poor (0.60-0.69)')

plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# =============================================================================
# COMPREHENSIVE COMPARISON (ALL METRICS)
# =============================================================================

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

metrics = ['R²', 'Precision', 'Recall', 'F1-Score', 'AUC']
titles = ['R² Score Comparison', 'Precision Comparison', 'Recall Comparison', 
          'F1-Score Comparison', 'AUC Comparison']

for i, (metric, title) in enumerate(zip(metrics, titles)):
    if i < 5:  # We have 5 metrics
        row = i // 3
        col = i % 3
        
        bars = axes[row, col].bar(df_results['Algorithm'], df_results[metric], color=colors)
        axes[row, col].set_title(title)
        axes[row, col].set_ylabel(metric)
        axes[row, col].set_ylim(0, 1)
        
        # Add value labels
        for bar, value in zip(bars, df_results[metric]):
            height = bar.get_height()
            axes[row, col].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                               f'{value:.3f}', ha='center', va='bottom', fontsize=9)
        
        # Add success criteria line
        axes[row, col].axhline(y=0.8, color='red', linestyle='--', alpha=0.5)
        axes[row, col].grid(True, alpha=0.3)
        axes[row, col].tick_params(axis='x', rotation=45)

# Remove empty subplot
axes[1, 2].axis('off')

plt.tight_layout()
plt.show()

# =============================================================================
# SUMMARY TABLE VISUALIZATION
# =============================================================================

fig, ax = plt.subplots(figsize=(12, 6))
ax.axis('tight')
ax.axis('off')

# Create table
table_data = []
for i, row in df_results.iterrows():
    table_data.append([row['Algorithm'], f"{row['R²']:.4f}", f"{row['Precision']:.4f}", 
                      f"{row['Recall']:.4f}", f"{row['F1-Score']:.4f}", f"{row['AUC']:.4f}"])

table = ax.table(cellText=table_data,
                colLabels=['Algorithm', 'R² Score', 'Precision', 'Recall', 'F1-Score', 'AUC'],
                cellLoc='center',
                loc='center',
                bbox=[0, 0, 1, 1])

# Style the table
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 1.5)

# Color header row
for i in range(len(df_results.columns)):
    table[(0, i)].set_facecolor('#40466e')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Color data rows alternately
for i in range(1, len(df_results) + 1):
    for j in range(len(df_results.columns)):
        if i % 2 == 0:
            table[(i, j)].set_facecolor('#f1f1f2')

plt.title('Model Performance Comparison Summary', fontsize=16, fontweight='bold', pad=20)
plt.show()

print("="*60)
print("EVALUATION CHARTS GENERATED")
print("="*60)
print("Charts created:")
print("1. R² Score Comparison")
print("2. Precision Comparison") 
print("3. Recall Comparison")
print("4. F1-Score Comparison")
print("5. AUC Comparison with Performance Grades")
print("6. Comprehensive Multi-Metric Comparison")
print("7. Summary Performance Table")
print("\nReplace the model_results data with your actual results from the modeling section.")